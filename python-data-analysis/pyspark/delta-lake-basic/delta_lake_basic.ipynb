{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87caaa4f-861a-4627-b6d9-1b1679892b0b",
   "metadata": {},
   "source": [
    "### 必要モジュールのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841c3a39-3b83-4be1-babe-b4e62ca62994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.17.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543179b-8497-4f43-8fd6-0f3e2eb913c8",
   "metadata": {},
   "source": [
    "### Sparkセッションの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b162c1-9d67-4945-8c1d-db0bd90fe46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# SparkSessionの初期化\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"DeltaLakeExample\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "# Delta Lakeで動作するように設定\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b0467-0ff5-4eae-8f0d-2db6f1979179",
   "metadata": {},
   "source": [
    "### データの入出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d59f2e-f712-46fb-b1b6-d70fa012912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|  Taro| 25|\n",
      "|Hanako| 30|\n",
      "|  Yuki| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# ===== データの作成\n",
    "# スキーマを定義\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# データを作成\n",
    "data = [(\"Taro\", 25), (\"Hanako\", 30), (\"Yuki\", 20)]\n",
    "\n",
    "# スキーマを指定してデータフレームを生成\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# DataFrameの内容を表示\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4a889c-6f0a-4252-b5aa-513ffd2fc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの書き込み\n",
    "(df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"./delta-table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703df285-2741-420b-95b6-541f0604b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Hanako| 30|\n",
      "|  Taro| 25|\n",
      "|  Yuki| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "df_read = (spark.read\n",
    "                .format(\"delta\")\n",
    "                .load(\"./delta-table\")\n",
    ")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73129927-274d-4992-9c2c-ba4fb717db9d",
   "metadata": {},
   "source": [
    "### タイムトラベルを使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93335b4f-7ca6-4b74-90a7-103bd1f5e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|  Taro| 25|\n",
      "|Hanako| 30|\n",
      "|  Yuki| 20|\n",
      "|  Jiro| 40|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データを更新する\n",
    "new_row = spark.createDataFrame([(\"Jiro\", 40)], schema)\n",
    "\n",
    "# データフレームに追加\n",
    "df = df.union(new_row)\n",
    "df.show()\n",
    "\n",
    "# データの更新\n",
    "(df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"./delta-table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b23323-ad6c-481a-8e3e-5a546f2739bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2023-12-09 10:09:36.239|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numFiles -> 5, numOutputRows -> 4, numOutputBytes -> 3158}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2023-12-09 10:09:30.767|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 4, numOutputRows -> 3, numOutputBytes -> 2468}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n",
      "+-------+\n",
      "|version|\n",
      "+-------+\n",
      "|1      |\n",
      "|0      |\n",
      "+-------+\n",
      "\n",
      "+-----------------------+\n",
      "|timestamp              |\n",
      "+-----------------------+\n",
      "|2023-12-09 10:09:36.239|\n",
      "|2023-12-09 10:09:30.767|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== タイムトラベルを使用する\n",
    "from delta import DeltaTable\n",
    "\n",
    "# Delta Lakeの履歴情報を取得する\n",
    "delta_table = DeltaTable.forPath(spark, \"./delta-table\")\n",
    "history = delta_table.history()\n",
    "\n",
    "# すべての履歴を表示\n",
    "history.show(truncate=False)\n",
    "# バージョンのみ選択して表示\n",
    "history.select(\"version\").show(truncate=False)\n",
    "# タイムスタンプのみ選択して表示\n",
    "history.select(\"timestamp\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933748e0-befa-4dd1-b892-db6d616a3e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Hanako| 30|\n",
      "|  Taro| 25|\n",
      "|  Yuki| 20|\n",
      "+------+---+\n",
      "\n",
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Hanako| 30|\n",
      "|  Taro| 25|\n",
      "|  Yuki| 20|\n",
      "|  Jiro| 40|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 特定のバージョンを使用してアクセスする\n",
    "df_version0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"./delta-table\")\n",
    "df_version0.show()\n",
    "\n",
    "df_version1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"./delta-table\")\n",
    "df_version1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1079bd-95c9-4aac-a1ca-3e955170c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Hanako| 30|\n",
      "|  Taro| 25|\n",
      "|  Yuki| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# タイムスタンプを使用してアクセスする\n",
    "timestamp = \"2023-12-09 10:09:30.767\"\n",
    "\n",
    "df_timestamp = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp).load(\"./delta-table\")\n",
    "df_timestamp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e350face-ec7a-4fc3-9f5c-a506c3c13903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|2      |2023-12-09 10:09:51.892|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2084}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|1      |2023-12-09 10:09:36.239|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numFiles -> 5, numOutputRows -> 4, numOutputBytes -> 3158}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2023-12-09 10:09:30.767|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 4, numOutputRows -> 3, numOutputBytes -> 2468}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データを特定のバージョンにロールバックする\n",
    "# バージョンを指定する\n",
    "rollback_version = 0\n",
    "\n",
    "# 該当データを読み込む\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", rollback_version).load(\"./delta-table\")\n",
    "\n",
    "# テーブルを上書きする\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"./delta-table\")\n",
    "\n",
    "# Delta Lakeの履歴情報を取得する\n",
    "delta_table = DeltaTable.forPath(spark, \"./delta-table\")\n",
    "history = delta_table.history()\n",
    "history.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e12329-5656-44f9-8968-4fddeaa42dae",
   "metadata": {},
   "source": [
    "### スキーマの進化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e7bbdc-2b46-46c6-ac62-80b8d65e0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|  name|age|flg|\n",
      "+------+---+---+\n",
      "|Hanako| 30|  1|\n",
      "|  Taro| 25|  1|\n",
      "|  Yuki| 20|  1|\n",
      "+------+---+---+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 38964833-dc9b-49a6-bfe5-b0c9142989fa).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- name: string (nullable = true)\n-- age: integer (nullable = true)\n\n\nData schema:\nroot\n-- name: string (nullable = true)\n-- age: integer (nullable = true)\n-- flg: integer (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# テーブルを上書きする ←これはエラーになる\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./delta-table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 38964833-dc9b-49a6-bfe5-b0c9142989fa).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- name: string (nullable = true)\n-- age: integer (nullable = true)\n\n\nData schema:\nroot\n-- name: string (nullable = true)\n-- age: integer (nullable = true)\n-- flg: integer (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Delta Lakeに列を追加する\n",
    "df = df.withColumn(\"flg\", lit(1))\n",
    "df.show()\n",
    "\n",
    "# テーブルを上書きする ←これはエラーになる\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"./delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a66127-eb2a-4992-9875-1e31fb1f480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定の書き込みにのみスキーマの進化を適用したい場合\n",
    "(df.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"./delta-table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a8eeff8-ae2c-4e07-bd45-770e7dce1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+-------+\n",
      "|  name|age|flg|country|\n",
      "+------+---+---+-------+\n",
      "|Hanako| 30|  1|    jpn|\n",
      "|  Taro| 25|  1|    jpn|\n",
      "|  Yuki| 20|  1|    jpn|\n",
      "+------+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# スキーマの進化を有効化\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "# Delta Lakeに列を追加する\n",
    "df = df.withColumn(\"country\", lit(\"jpn\"))\n",
    "df.show()\n",
    "\n",
    "# テーブルを上書きする\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"./delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd08a9-4e54-4081-9f9d-069251ccbc53",
   "metadata": {},
   "source": [
    "### メタデータの管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93e2eaf3-ea1c-4c94-9ec9-a000e2848b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブル情報の取得\n",
    "delta_table = DeltaTable.forPath(spark, \"./delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e55f9d-4da4-44ac-9c1b-749f442b90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+-------+\n",
      "|  name|age|flg|country|\n",
      "+------+---+---+-------+\n",
      "|Hanako| 30|  1|    jpn|\n",
      "|  Yuki| 20|  1|    jpn|\n",
      "|  Taro| 25|  1|    jpn|\n",
      "+------+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テーブル情報の表示\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6593cbe3-2be1-41de-b77c-80c50bef6d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "|5      |2023-12-09 10:10:00.117|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |4          |Serializable  |false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3452}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|4      |2023-12-09 10:09:58.606|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |3          |Serializable  |false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3452}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|3      |2023-12-09 10:09:54.709|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |2          |Serializable  |false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3452}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|2      |2023-12-09 10:09:51.892|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |1          |Serializable  |false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2084}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|1      |2023-12-09 10:09:36.239|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numFiles -> 5, numOutputRows -> 4, numOutputBytes -> 3158}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "|0      |2023-12-09 10:09:30.767|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 4, numOutputRows -> 3, numOutputBytes -> 2468}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テーブルの履歴とバージョン\n",
    "delta_table.history().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8561d47d-1927-4ef7-a825-40516c36a0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+----------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name|description|location                                                                                                  |createdAt              |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+----+-----------+----------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |38964833-dc9b-49a6-bfe5-b0c9142989fa|NULL|NULL       |file:/home/jovyan/work/python-tech-sample-source/python-data-analysis/pyspark/delta-lake-basic/delta-table|2023-12-09 10:09:28.235|2023-12-09 10:10:00.117|[]              |3       |3452       |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+----+-----------+----------------------------------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テーブルのファイル情報\n",
    "spark.sql(\"DESCRIBE DETAIL './delta-table'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07365e4c-b95d-4736-94d0-1b3abf577db2",
   "metadata": {},
   "source": [
    "### セッションを終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "973c64cd-ca9f-45fa-b574-d71c991a2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSessionを終了\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368ae38-0554-4740-bac8-cd87fef0105f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
